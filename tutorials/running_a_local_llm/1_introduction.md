# Running a local LLM

As you start thinking about your projects, one of the big concerns that you might have is, _''I don't have the GPU resources to use (let alone fine-tune) a GenAI model.''_ Think again! _Training_ or _fine-tuning_ a model is not required for this course, and yes it is the case that you probably don't have the necessary resources to do so on your own machine. But _using_ an existing model is more than likely possible.

## What model(s) can you use on your local machine?
Remember that for this course, your projects can be very small. So it is OK to use a very small model. Like _llama 3.2 1B_. This is the 1-billion parameter version of Meta's llama family of LLMs. Relatively speaking, it's _very_ small. But that doesn't mean that you shouldn't use it for your projects. If you want, you can try some of the larger models as well, like a 7B version. Whatever works for you.

Remember that the focus of this course is on evaluating the systems that use these models, not the models themselves. A prototype system that uses small models is perfectly fine for you to use.

## What's next
There are lots of different ways that you can run these kinds of small models. In the next page, we'll talk about how to install a _very_ small model.